# -*- coding: utf-8 -*-
"""DL Assignment 2.2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vAbGczge5KlPqRJBT2NKbzvdfRI58Wgp
"""

# QUESTION 2

# Step 1: Install and configure libraries
!pip install transformers datasets --quiet

import os
os.environ["WANDB_DISABLED"] = "true"  # ðŸ”‡ Disable Weights & Biases logging

# Step 2: Import required modules
import torch
from google.colab import files
from datasets import Dataset
from transformers import (
    GPT2Tokenizer, GPT2LMHeadModel,
    DataCollatorForLanguageModeling,
    Trainer, TrainingArguments
)

# Step 3: Upload text file
uploaded_files = files.upload()
filename = next(iter(uploaded_files))  # Get uploaded filename

# Step 4: Read file contents
with open(filename, "r", encoding="utf-8") as file:
    text_lines = [line.strip() for line in file.readlines() if line.strip()]

# Step 5: Create a Hugging Face dataset from text
text_dataset = Dataset.from_dict({"text": text_lines})

# Step 6: Load tokenizer and model
base_model = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(base_model)
tokenizer.pad_token = tokenizer.eos_token  # Use EOS token for padding

model = GPT2LMHeadModel.from_pretrained(base_model)

# Step 7: Tokenization function
def encode_batch(batch):
    return tokenizer(batch["text"], truncation=True, padding="max_length", max_length=128)

encoded_dataset = text_dataset.map(encode_batch, batched=True)

# Step 8: Prepare data collator for causal language modeling
collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # Causal LM setting
)

# Step 9: Define training parameters
train_config = TrainingArguments(
    output_dir="./gpt2-finetuned-lyrics",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=200,
    logging_steps=20,
    overwrite_output_dir=True,
    save_total_limit=1,
    prediction_loss_only=True,
    logging_dir="./training_logs"
)

# Step 10: Initialize the Trainer
trainer = Trainer(
    model=model,
    args=train_config,
    train_dataset=encoded_dataset,
    tokenizer=tokenizer,
    data_collator=collator
)

# Step 11: Start training
trainer.train()

# Step 12: Save the fine-tuned model and tokenizer
model.save_pretrained("gpt2-finetuned-lyrics")
tokenizer.save_pretrained("gpt2-finetuned-lyrics")

from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel

# Load the model and tokenizer from local directory (after training)
local_model_dir = "./gpt2-finetuned-lyrics"  # match this with your training save path

tokenizer = GPT2Tokenizer.from_pretrained(local_model_dir)
model = GPT2LMHeadModel.from_pretrained(local_model_dir)

# Create text generation pipeline using local model
text_generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Get prompt from user
user_input = input("Type a starting line for your song: ")

# Generate lyrics based on prompt
output = text_generator(user_input, max_new_tokens=50)

# Display the generated text
print("\nðŸŽµ Generated Lyrics:")
print(output[0]['generated_text'])